{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.probability import FreqDist\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"C:\\\\Users\\\\gh68930\\\\nltk_data\")\n",
    "text_field = 'QQ_NPS_COMMENT'\n",
    "score_field = 'QQ_KEYMETRIC_LTR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    path = os.getwd() + path\n",
    "    csv_files = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "    \n",
    "    all_detractors = pd.DataFrame(columns=['comment'])\n",
    "    for f in csv_files:\n",
    "        df.pd.read_csv(f)\n",
    "        df['npsscore'] = df.filter(regex=score_field).iloc[:,0].str.replace(r\"[^0-9]\",\"\").astype('int')\n",
    "        df = df[df['LanguageID']=='EN']\n",
    "        df = df[df['npsscore']<=6].filter(regex=text_field)\n",
    "        df = df.dropna()df.columns = ['comment']\n",
    "        all_detractors = pd.concat([all_detractors, df], ignore_index = True)\n",
    "    return all_detractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = read_data(\"\\\\verbatim\\\\train\")\n",
    "df_validation_raw = read_data('\\\\verbatim\\\\test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_raw.copy(deep=True)\n",
    "df_validation = df_validation_raw.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"comment\"] = df_train[\"comment\"].str.replace(r\"[!#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]\",\"\")\n",
    "df_train[\"comment\"] = df_train[\"comment\"].str.replace(r\"[^A-Za-z]\", \" \")\n",
    "df_train[\"comment\"] = df_train[\"comment\"].str.lower()\n",
    "df_train[\"comment\"] = df_train[\"comment\"].apply(lambda s: re.sub(' +', ' ', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation[\"comment\"] = df_validation[\"comment\"].str.replace(r\"[!#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]\",\"\")\n",
    "df_validation[\"comment\"] = df_validation[\"comment\"].str.replace(r\"[^A-Za-z]\", \" \")\n",
    "df_validation[\"comment\"] = df_validation[\"comment\"].str.lower()\n",
    "df_validation[\"comment\"] = df_validation[\"comment\"].apply(lambda s: re.sub(' +', ' ', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"comment\"] = df_train[\"comment\"].apply(lambda s: re.sub('dept', 'department', s))\n",
    "df_validation[\"comment\"] = df_validation[\"comment\"].apply(lambda s: re.sub('dept', 'department', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    word_list = sentence.split(' ')\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w, wordnet.VERB) for w in word_list])\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"comment\"] = df_train[\"comment\"].apply(lemmatize_sentence)\n",
    "df_validation[\"comment\"] = df_validation[\"comment\"].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2[\"comment\"] = df_train[\"comment\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation2[\"comment\"] = df_validation[\"comment\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "#### Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collections.BigramAssocMeasures()\n",
    "bi_finder = nltk.collections.BigramCollocationFinder.from_documents([comment.split() for comment in df_train2.comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_finder = apply_freq_filter(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_scores = bi_finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [' '.join(i[0]) for i in bigram_scores if i[1] >= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collections.TrigramAssocMeasures()\n",
    "tri_finder = nltk.collections.TrigramCollocationFinder.from_documents([comment.split() for comment in df_train2.comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_finder = apply_freq_filter(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_scores = tri_finder.score_ngrams(trigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [' '.join(i[0]) for i in trigram_scores if i[1] >= 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace N-grams in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_ngrams(x):\n",
    "    for gram in bigrams:\n",
    "        x = x.replace(gram, '_'.join(gram.split()))\n",
    "    for gram in trigrams:\n",
    "        x = x.replace(gram, '_'.join(gram.split()))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.comment = df_train2.comment.map(lambda x:replace_ngrams(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation2.comment = df_validation2.comment.map(lambda x:replace_ngrams(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_df = 0.95, min_df = 100)\n",
    "\n",
    "cv_train = cv.fit_transform(df_train2[\"comment\"].tolist())\n",
    "cv_train_feature_names = cv.get_feature_names()\n",
    "\n",
    "cv_validation = cv.transform(df_validation2[\"comment\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "no_topics = 15\n",
    "hyper_alpha = 0.8\n",
    "hyper_beta = 0.01\n",
    "this_random_state = 53\n",
    "start = time.time()\n",
    "lda_detractors = LatentDirichletAllocation(n_components = no_topics,\n",
    "                                          doc_topic_prior = hyper_alpha,\n",
    "                                          topic_word_prior = hyper_beta,\n",
    "                                          max_iter = 300,\n",
    "                                          learning_method = 'batch',\n",
    "                                          #learning_offset = 75,\n",
    "                                          random_state = this_random_state).fit(cv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_detractors.perplexity(cv_train))\n",
    "print(lda_detractors.score(cv_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_detractors.perplexity(cv_validation))\n",
    "print(lda_detractors.score(cv_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get topic correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lda_detractors.transform(X = cv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = pd.DataFrame()\n",
    "for num in range(no_topics):\n",
    "    my_column_name = \"Topic_\" + str(num) + \"_score\"\n",
    "    corr_data = corr_data.assign(**{my_column_name: predictions[:, num]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "corr = corr_data.corr()\n",
    "\n",
    "sns.set(style = \"white\")\n",
    "mask = np.zeros_like(corr, dtype = np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplot(figsize = (11,9))\n",
    "sns.heatmap(corr, mask = mask, vmax = 0.3, center = 0,\n",
    "           square = True, linewidths = 0.5, cbar_kws = {\"shrink\": 0.5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic % d: \" % (topic_idx), end = '')\n",
    "        for i in topic.argsort()[: -no_top_words - 1:-1]:\n",
    "            if int(topic[i]) > 1:\n",
    "                print(feature_names[i], end = ', ')\n",
    "        print('\\n', end = '')\n",
    "        \n",
    "no_top_words = 25\n",
    "\n",
    "display_topics(lda_detractors, cv_train_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important words for Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(columns = ['Topic', 'Words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words['Topic'] = [i for i in range(no_topics)]\n",
    "df_words['Words'] = df_words['Words'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, topic in enumerate(lda_detractors.components_):\n",
    "    print(f\"The top 15 words for Topic {index}\")\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    df_words.at[index, 'Words'] = [cv.get_feature_names()[i] for i in topic_argsort()[-20:]]\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging Train & Validation topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results_train = lda_detractors.transform(cv_train)\n",
    "print(topic_results_train.shape)\n",
    "df_train_raw['Topic'] = topic_results_train.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results_validation = lda_detractors.transform(cv_validation)\n",
    "print(topic_results_validation.shape)\n",
    "df_validation_raw['Topic'] = topic_results_validation.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the labeled data, Model and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('LDA_detractors.xlsx', engine = 'xlsxwriter')\n",
    "df_train_raw.to_excel(writer, 'Train')\n",
    "df_validation_raw.to_excel(writer, 'Validation')\n",
    "df_words.to_excel.to_excel(writer, 'Words')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "vectorizer_name = os.getcwd() + '\\\\verbatim\\\\detractor_vectorizer.pkl'\n",
    "with open(vectorizer_name, 'wb') as file:\n",
    "    pickle.dump(cv, file)\n",
    "    \n",
    "pkl_filenam = os.getcwd() + '\\\\verbatim\\\\lda_detractor_model.pkl'\n",
    "with open(pkl_filenam, 'wb') as file:\n",
    "    pickle.dump(lda_detractors, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_end = time.time()\n",
    "print((code_end - code_start)/60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
